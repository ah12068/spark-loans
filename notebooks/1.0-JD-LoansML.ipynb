{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T12:48:43.510195Z",
     "start_time": "2020-05-26T12:48:43.360806Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import when, col\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T12:48:43.514096Z",
     "start_time": "2020-05-26T12:48:43.511619Z"
    }
   },
   "outputs": [],
   "source": [
    "path = f'../data/processed/loans_spark/part-*.csv'\n",
    "\n",
    "target = 'class'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T12:48:46.319999Z",
     "start_time": "2020-05-26T12:48:43.515898Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('LogisticRegression').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T12:48:46.332679Z",
     "start_time": "2020-05-26T12:48:46.321711Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_features(data, spark_session, target):\n",
    "    output_name = f'features'\n",
    "    features = VectorAssembler(\n",
    "    inputCols = [var for var in data.columns if var not in target],\n",
    "    outputCol = output_name\n",
    "    )\n",
    "    \n",
    "    output = features.transform(data)\n",
    "    \n",
    "    output.createTempView(output_name)\n",
    "    \n",
    "    finalised_data = spark_session.sql(f'''\n",
    "    select\n",
    "    features,\n",
    "    {target} as label\n",
    "    from {output_name}\n",
    "    ''')\n",
    "    \n",
    "    return finalised_data\n",
    "\n",
    "def weight_balance(data, labels):\n",
    "    ratio = 1-data.groupby('label').count().collect()[0][1] / data.groupby('label').count().collect()[1][1]\n",
    "    return when(labels == 1, ratio).otherwise(1*(1-ratio))\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_set, spark_session):\n",
    "    test_set_name = 'test'\n",
    "    evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "    test_preds = model.transform(test_set)\n",
    "    \n",
    "    auc = evaluator.evaluate(test_preds)\n",
    "    \n",
    "    test_preds.createTempView(test_set_name)\n",
    "    \n",
    "    metrics = spark_session.sql(f'''\n",
    "        select\n",
    "        round({auc}*100, 3) as auc,\n",
    "        round((true_positive / (true_positive + false_negative))*100, 3) as recall_pct,\n",
    "        round((true_positive / (true_positive + false_positive))*100, 3) as precision_pct,\n",
    "        *\n",
    "\n",
    "\n",
    "        from \n",
    "\n",
    "        ( select\n",
    "        round(avg(case when label=prediction then 1 else 0 end)*100, 3) as accuracy_pct,\n",
    "        sum(case when label=1 and prediction=1 then 1 else 0 end) as true_positive,\n",
    "        sum(case when label=0 and prediction=0 then 1 else 0 end) as true_negative,\n",
    "        sum(case when label=0 and prediction=1 then 1 else 0 end) as false_positive,\n",
    "        sum(case when label=1 and prediction=0 then 1 else 0 end) as false_negative\n",
    "\n",
    "        from {test_set_name} )\n",
    "            ''')\n",
    "    \n",
    "    metrics.show()\n",
    "    \n",
    "    return metrics\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T12:48:50.511036Z",
     "start_time": "2020-05-26T12:48:46.334099Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data = spark.read.csv(\n",
    "    path,\n",
    "    inferSchema=True,\n",
    "    header=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T12:48:50.576695Z",
     "start_time": "2020-05-26T12:48:50.514817Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- class: integer (nullable = true)\n",
      " |-- binary_term: double (nullable = true)\n",
      " |-- purpose_other: integer (nullable = true)\n",
      " |-- purpose_debt_consolidation: integer (nullable = true)\n",
      " |-- purpose_credit_card: integer (nullable = true)\n",
      " |-- home_ownership_own: integer (nullable = true)\n",
      " |-- home_ownership_other: integer (nullable = true)\n",
      " |-- home_ownership_mortgage: integer (nullable = true)\n",
      " |-- home_ownership_rent: integer (nullable = true)\n",
      " |-- employment_length_7to9: integer (nullable = true)\n",
      " |-- employment_length_10plus: integer (nullable = true)\n",
      " |-- employment_length_1to3: integer (nullable = true)\n",
      " |-- employment_length_less1: integer (nullable = true)\n",
      " |-- employment_length_4to6: integer (nullable = true)\n",
      " |-- inquiries_6m_1_inquiry: integer (nullable = true)\n",
      " |-- inquiries_6m_2plus_inquiry: integer (nullable = true)\n",
      " |-- inquiries_6m_no_inquiry: integer (nullable = true)\n",
      " |-- scaled_installment: double (nullable = true)\n",
      " |-- scaled_loan_amount: double (nullable = true)\n",
      " |-- scaled_interest_rate: double (nullable = true)\n",
      " |-- scaled_annual_income: double (nullable = true)\n",
      " |-- scaled_public_records: double (nullable = true)\n",
      " |-- scaled_delinquency_2y: double (nullable = true)\n",
      " |-- scaled_open_accounts: double (nullable = true)\n",
      " |-- scaled_debt_to_income: double (nullable = true)\n",
      " |-- scaled_credit_card_usage: double (nullable = true)\n",
      " |-- scaled_credit_card_balance: double (nullable = true)\n",
      " |-- scaled_total_current_balance: double (nullable = true)\n",
      " |-- scaled_nr_accounts: double (nullable = true)\n",
      " |-- scaled_credit_score: double (nullable = true)\n",
      " |-- scaled_credit_age_years: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T12:48:51.318073Z",
     "start_time": "2020-05-26T12:48:50.579581Z"
    }
   },
   "outputs": [],
   "source": [
    "data = get_features(data, spark, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T12:48:54.643657Z",
     "start_time": "2020-05-26T12:48:51.321529Z"
    }
   },
   "outputs": [],
   "source": [
    "data = data.withColumn('weights', weight_balance(data, col('label')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T12:48:54.990533Z",
     "start_time": "2020-05-26T12:48:54.647817Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------------------+\n",
      "|            features|label|           weights|\n",
      "+--------------------+-----+------------------+\n",
      "|(30,[1,6,9,15,16,...|    0|0.0496019035416857|\n",
      "|(30,[1,7,10,13,16...|    0|0.0496019035416857|\n",
      "|(30,[2,4,12,15,16...|    0|0.0496019035416857|\n",
      "|(30,[2,7,11,15,16...|    0|0.0496019035416857|\n",
      "|(30,[2,7,9,14,16,...|    0|0.0496019035416857|\n",
      "|(30,[1,7,8,13,16,...|    0|0.0496019035416857|\n",
      "|(30,[1,7,10,14,16...|    0|0.0496019035416857|\n",
      "|(30,[1,7,11,14,16...|    0|0.0496019035416857|\n",
      "|(30,[2,6,8,15,16,...|    0|0.0496019035416857|\n",
      "|(30,[1,7,11,14,16...|    0|0.0496019035416857|\n",
      "+--------------------+-----+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T12:48:55.039140Z",
     "start_time": "2020-05-26T12:48:54.992820Z"
    }
   },
   "outputs": [],
   "source": [
    "train, test = data.randomSplit([.8, .2], seed=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T12:48:55.104753Z",
     "start_time": "2020-05-26T12:48:55.043320Z"
    }
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(\n",
    "    featuresCol = data.columns[0],\n",
    "    labelCol = data.columns[1],\n",
    "    weightCol = data.columns[2],\n",
    "    maxIter = 100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T12:48:55.111480Z",
     "start_time": "2020-05-26T12:48:55.106649Z"
    }
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T12:48:55.126689Z",
     "start_time": "2020-05-26T12:48:55.114873Z"
    }
   },
   "outputs": [],
   "source": [
    "paramGrid = ParamGridBuilder() \\\n",
    ".addGrid(lr.regParam, [0.001, 0.01, 0.1, 1]) \\\n",
    ".addGrid(lr.elasticNetParam, [0.001, 0.01, 0.1, 1]) \\\n",
    ".build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T12:48:55.144468Z",
     "start_time": "2020-05-26T12:48:55.129486Z"
    }
   },
   "outputs": [],
   "source": [
    "model_tune = TrainValidationSplit(estimator=pipeline,\n",
    "                      estimatorParamMaps=paramGrid,\n",
    "                      evaluator=BinaryClassificationEvaluator(metricName='areaUnderPR'),\n",
    "                      trainRatio=0.8)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T12:50:52.380903Z",
     "start_time": "2020-05-26T12:48:55.148598Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = model_tune.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T12:50:55.095620Z",
     "start_time": "2020-05-26T12:50:52.384723Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-------------+------------+-------------+-------------+--------------+--------------+\n",
      "|   auc|recall_pct|precision_pct|accuracy_pct|true_positive|true_negative|false_positive|false_negative|\n",
      "+------+----------+-------------+------------+-------------+-------------+--------------+--------------+\n",
      "|73.172|    63.259|        9.851|      70.627|         1250|        28000|         11439|           726|\n",
      "+------+----------+-------------+------------+-------------+-------------+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metrics = evaluate_model(model, test, spark)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
