{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T13:14:48.865104Z",
     "start_time": "2020-05-20T13:14:48.697965Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType, DateType\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T13:14:48.871913Z",
     "start_time": "2020-05-20T13:14:48.867110Z"
    }
   },
   "outputs": [],
   "source": [
    "path = f'../data/raw/loans_raw.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T13:14:51.389638Z",
     "start_time": "2020-05-20T13:14:48.873557Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Loans').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T13:14:51.411397Z",
     "start_time": "2020-05-20T13:14:51.391019Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def drop_na_cols(data, pct):\n",
    "    rows = data.count()\n",
    "    null_counts = data.select(\n",
    "        [F.count(\n",
    "            F.when(\n",
    "                F.isnull(col), col)\n",
    "        ).alias(col) for col in data.columns]\n",
    "    )\n",
    "    \n",
    "    null_counts = null_counts.toPandas()\n",
    "    null_counts = (null_counts/rows).ge(pct).all()\n",
    "    null_cols = null_counts[null_counts == True].keys()\n",
    "    \n",
    "    \n",
    "    \n",
    "    return data.select([col for col in data.columns if col not in null_cols])\n",
    "\n",
    "def lower_case_cols(data):\n",
    "    data_dtypes = {col[0]: col[1] for col in data.dtypes}\n",
    "    \n",
    "    for column in data_dtypes.keys():\n",
    "        if data_dtypes[column] == 'string':\n",
    "            data = data.withColumn(column, F.lower(F.col(column)))\n",
    "    \n",
    "    return data\n",
    "    \n",
    "def remove_whitespace(data):\n",
    "    data_dtypes = {col[0]: col[1] for col in data.dtypes}\n",
    "    \n",
    "    for column in data_dtypes.keys():\n",
    "        if data_dtypes[column] == 'string':\n",
    "            data = data.withColumn(column, F.lower(F.col(column)))\n",
    "        \n",
    "    return data\n",
    "\n",
    "def make_col_numeric(data, column):\n",
    "    return data.withColumn(column, data[column].cast(IntegerType()))\n",
    "\n",
    "def truncate_credit_line(data, column):\n",
    "    return data.withColumn(column, F.split(F.col(column), '-')[1])\n",
    "\n",
    "def truncate_term(data, column):\n",
    "    return data.withColumn(column, F.split(F.col(column), ' ')[1])\n",
    "\n",
    "def categorise_employment_length(data, spark_session):\n",
    "    name = 'df_el'\n",
    "    data.createTempView(name)\n",
    "    \n",
    "    data = spark_session.sql(\n",
    "    f\"\"\"\n",
    "    \n",
    "    select\n",
    "    account_id,\n",
    "    installment,\n",
    "    loan_amount,\n",
    "    interest_rate,\n",
    "    term,\n",
    "    purpose,\n",
    "    issue_date,\n",
    "    title,\n",
    "    home_ownership,\n",
    "    annual_income,\n",
    "\n",
    "    case when employment_length in ('10+ years') then '10plus'\n",
    "    when employment_length in ('< 1 year') then 'less1'\n",
    "    when employment_length in ('1 year', '2 years', '3 years') then '1to3'\n",
    "    when employment_length in ('4 years', '5 years', '6 years') then '4to6'\n",
    "    when employment_length in ('7 years', '8 years', '9 years') then '7to9'\n",
    "    else null\n",
    "    end as employment_length,\n",
    "\n",
    "    job_title,\n",
    "    earliest_credit_line,\n",
    "    public_records,\n",
    "    delinquency_2y,\n",
    "    inquiries_6m,\n",
    "    open_accounts,\n",
    "    debt_to_income,\n",
    "    credit_card_usage,\n",
    "    credit_card_balance,\n",
    "    total_current_balance,\n",
    "    nr_accounts,\n",
    "    loan_status,\n",
    "    amount_payed,\n",
    "    year,\n",
    "    district,\n",
    "    postcode_district,\n",
    "    credit_score\n",
    "\n",
    "    from {name}\n",
    "    \"\"\"\n",
    "    )\n",
    "    \n",
    "    return data\n",
    "\n",
    "def categorise_home_ownership(data, spark):\n",
    "    name = 'df_ho'\n",
    "    data.createTempView(name)\n",
    "    \n",
    "    data = spark.sql(\n",
    "    f\"\"\"\n",
    "    select\n",
    "    account_id,\n",
    "    installment,\n",
    "    loan_amount,\n",
    "    interest_rate,\n",
    "    term,\n",
    "    purpose,\n",
    "    issue_date,\n",
    "    title,\n",
    "    \n",
    "    case when home_ownership in ('mortgage', 'rent', 'own') then home_ownership\n",
    "    else 'other'\n",
    "    end as home_ownership,\n",
    "    \n",
    "    annual_income,\n",
    "    employment_length,\n",
    "    job_title,\n",
    "    earliest_credit_line,\n",
    "    public_records,\n",
    "    delinquency_2y,\n",
    "    inquiries_6m,\n",
    "    open_accounts,\n",
    "    debt_to_income,\n",
    "    credit_card_usage,\n",
    "    credit_card_balance,\n",
    "    total_current_balance,\n",
    "    nr_accounts,\n",
    "    loan_status,\n",
    "    amount_payed,\n",
    "    year,\n",
    "    district,\n",
    "    postcode_district,\n",
    "    credit_score\n",
    "    \n",
    "    \n",
    "    from {name}\n",
    "    \"\"\"\n",
    "    )\n",
    "    \n",
    "    return data\n",
    "\n",
    "def categorise_inquiry(data, spark):\n",
    "    name = 'df_inq'\n",
    "    data.createTempView(name)\n",
    "    \n",
    "    data = spark.sql(f\"\"\"\n",
    "    select\n",
    "    \n",
    "    account_id,\n",
    "    installment,\n",
    "    loan_amount,\n",
    "    interest_rate,\n",
    "    term,\n",
    "    purpose,\n",
    "    issue_date,\n",
    "    title,\n",
    "    home_ownership,\n",
    "    annual_income,\n",
    "    employment_length,\n",
    "    job_title,\n",
    "    earliest_credit_line,\n",
    "    public_records,\n",
    "    delinquency_2y,\n",
    "    \n",
    "    case when inquiries_6m = 0 then 'no_inquiry'\n",
    "    when inquiries_6m = 1 then '1_inquiry'\n",
    "    else '2plus_inquiry'\n",
    "    end as inquiries_6m,\n",
    "    \n",
    "    open_accounts,\n",
    "    debt_to_income,\n",
    "    credit_card_usage,\n",
    "    credit_card_balance,\n",
    "    total_current_balance,\n",
    "    nr_accounts,\n",
    "    loan_status,\n",
    "    amount_payed,\n",
    "    year,\n",
    "    district,\n",
    "    postcode_district,\n",
    "    credit_score\n",
    "    \n",
    "    from {name}\n",
    "    \"\"\"\n",
    "    )\n",
    "    return data\n",
    "\n",
    "def categorise_purpose(data, spark):\n",
    "    name = 'df_purpose'\n",
    "    data.createTempView(name)\n",
    "    \n",
    "    data = spark.sql(f\"\"\"\n",
    "    select\n",
    "    \n",
    "    account_id,\n",
    "    installment,\n",
    "    loan_amount,\n",
    "    interest_rate,\n",
    "    term,\n",
    "    \n",
    "    case when purpose in ('debt_consolidation', 'credit_card') then purpose\n",
    "    else 'other'\n",
    "    end as purpose,\n",
    "    \n",
    "    issue_date,\n",
    "    title,\n",
    "    home_ownership,\n",
    "    annual_income,\n",
    "    employment_length,\n",
    "    job_title,\n",
    "    earliest_credit_line,\n",
    "    public_records,\n",
    "    delinquency_2y,\n",
    "    inquiries_6m,\n",
    "    open_accounts,\n",
    "    debt_to_income,\n",
    "    credit_card_usage,\n",
    "    credit_card_balance,\n",
    "    total_current_balance,\n",
    "    nr_accounts,\n",
    "    loan_status,\n",
    "    amount_payed,\n",
    "    year,\n",
    "    district,\n",
    "    postcode_district,\n",
    "    credit_score\n",
    "    \n",
    "    from {name}    \n",
    "    \"\"\"\n",
    "    )\n",
    "    \n",
    "    return data\n",
    "\n",
    "def impute_column(data, spark_session, grouper, column_to_fill):\n",
    "    name = 'df_tcb'\n",
    "    data.createTempView(name)\n",
    "    \n",
    "    medians = data.groupBy(grouper).agg(F.expr(f'percentile_approx({column_to_fill}, 0.5)').alias(f'median_{column_to_fill}'))\n",
    "    medians_name = 'df_median'\n",
    "    medians.createTempView(medians_name)\n",
    "    \n",
    "    imputed_data = spark_session.sql(f\"\"\"\n",
    "    select\n",
    "    \n",
    "    account_id,\n",
    "    installment,\n",
    "    loan_amount,\n",
    "    interest_rate,\n",
    "    term,\n",
    "    purpose,\n",
    "    issue_date,\n",
    "    title,\n",
    "    home_ownership,\n",
    "    annual_income,\n",
    "    employment_length,\n",
    "    job_title,\n",
    "    earliest_credit_line,\n",
    "    public_records,\n",
    "    delinquency_2y,\n",
    "    inquiries_6m,\n",
    "    open_accounts,\n",
    "    debt_to_income,\n",
    "    credit_card_usage,\n",
    "    credit_card_balance,\n",
    "    \n",
    "    case when total_current_balance is null and {medians_name}.median_{column_to_fill} is not null then {medians_name}.median_{column_to_fill}\n",
    "    else total_current_balance\n",
    "    end as total_current_balance,\n",
    "    \n",
    "    nr_accounts,\n",
    "    loan_status,\n",
    "    amount_payed,\n",
    "    year,\n",
    "    {name}.district,\n",
    "    postcode_district,\n",
    "    credit_score\n",
    "    \n",
    "    from {name}\n",
    "    left join {medians_name} on {medians_name}.district = {name}.district\n",
    "    \n",
    "    \"\"\"\n",
    "    )\n",
    "    \n",
    "    return imputed_data\n",
    "\n",
    "def create_credit_age(data, spark_session, year):\n",
    "    name = 'df_credit_age'\n",
    "    data.createTempView(name)\n",
    "    \n",
    "    data = spark.sql(f\"\"\"\n",
    "    select\n",
    "    *,\n",
    "    \n",
    "    ({year} - earliest_credit_line) as credit_age_years\n",
    "    \n",
    "    from {name}\n",
    "    \"\"\"\n",
    "    )\n",
    "    \n",
    "    return data\n",
    "\n",
    "def create_binary_class(data, spark_session):\n",
    "    name = 'df_binary_class'\n",
    "    data.createTempView(name)\n",
    "    \n",
    "    data = spark_session.sql(f\"\"\"\n",
    "    select\n",
    "    \n",
    "    *,\n",
    "    \n",
    "    case when loan_status in ('fully paid') then 0\n",
    "    when loan_status in ('ongoing') then null\n",
    "    \n",
    "    when loan_status in ('default', 'charged_off', 'late (> 90 days)') then 1\n",
    "    else null\n",
    "    end as class\n",
    "    \n",
    "    from {name}\n",
    "    \"\"\"\n",
    "    )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T13:14:55.532916Z",
     "start_time": "2020-05-20T13:14:51.413076Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data = spark.read.csv(\n",
    "    path,\n",
    "    inferSchema=True,\n",
    "    header=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T13:14:55.552224Z",
     "start_time": "2020-05-20T13:14:55.533981Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- account_id: integer (nullable = true)\n",
      " |-- installment: double (nullable = true)\n",
      " |-- loan_amount: double (nullable = true)\n",
      " |-- interest_rate: double (nullable = true)\n",
      " |-- term: string (nullable = true)\n",
      " |-- purpose: string (nullable = true)\n",
      " |-- issue_date: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- home_ownership: string (nullable = true)\n",
      " |-- annual_income: string (nullable = true)\n",
      " |-- employment_length: string (nullable = true)\n",
      " |-- job_title: string (nullable = true)\n",
      " |-- earliest_credit_line: string (nullable = true)\n",
      " |-- public_records: string (nullable = true)\n",
      " |-- last_record_months: string (nullable = true)\n",
      " |-- last_delinquency_months: string (nullable = true)\n",
      " |-- last_derog_months: string (nullable = true)\n",
      " |-- delinquency_2y: string (nullable = true)\n",
      " |-- inquiries_6m: string (nullable = true)\n",
      " |-- open_accounts: string (nullable = true)\n",
      " |-- debt_to_income: string (nullable = true)\n",
      " |-- credit_card_usage: string (nullable = true)\n",
      " |-- credit_card_balance: string (nullable = true)\n",
      " |-- total_current_balance: string (nullable = true)\n",
      " |-- nr_accounts: string (nullable = true)\n",
      " |-- loan_status: string (nullable = true)\n",
      " |-- amount_payed: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- district: string (nullable = true)\n",
      " |-- postcode_district: string (nullable = true)\n",
      " |-- credit_score: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T13:14:57.862276Z",
     "start_time": "2020-05-20T13:14:55.554265Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = drop_na_cols(data=data, pct=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T13:14:58.074585Z",
     "start_time": "2020-05-20T13:14:57.864314Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = lower_case_cols(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T13:14:58.243567Z",
     "start_time": "2020-05-20T13:14:58.076597Z"
    }
   },
   "outputs": [],
   "source": [
    "data = remove_whitespace(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T13:14:58.258190Z",
     "start_time": "2020-05-20T13:14:58.245035Z"
    }
   },
   "outputs": [],
   "source": [
    "data = make_col_numeric(data, 'credit_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T13:14:58.267483Z",
     "start_time": "2020-05-20T13:14:58.259460Z"
    }
   },
   "outputs": [],
   "source": [
    "data = make_col_numeric(data, 'annual_income')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T13:14:58.319667Z",
     "start_time": "2020-05-20T13:14:58.269392Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = truncate_credit_line(data, 'earliest_credit_line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T13:14:58.645631Z",
     "start_time": "2020-05-20T13:14:58.321111Z"
    }
   },
   "outputs": [],
   "source": [
    "data = categorise_employment_length(data, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T13:14:58.740046Z",
     "start_time": "2020-05-20T13:14:58.647450Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = categorise_home_ownership(data, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T13:14:58.830103Z",
     "start_time": "2020-05-20T13:14:58.759461Z"
    }
   },
   "outputs": [],
   "source": [
    "data.createTempView('dft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T13:14:58.911175Z",
     "start_time": "2020-05-20T13:14:58.831549Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = categorise_inquiry(data, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T13:14:58.993648Z",
     "start_time": "2020-05-20T13:14:58.912670Z"
    }
   },
   "outputs": [],
   "source": [
    "data = categorise_purpose(data, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T13:14:59.242757Z",
     "start_time": "2020-05-20T13:14:58.995585Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = impute_column(data, spark, 'district', 'total_current_balance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T13:14:59.378710Z",
     "start_time": "2020-05-20T13:14:59.244168Z"
    }
   },
   "outputs": [],
   "source": [
    "data = create_credit_age(data, spark, 2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T13:14:59.512907Z",
     "start_time": "2020-05-20T13:14:59.380458Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = create_binary_class(data, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T13:14:59.550473Z",
     "start_time": "2020-05-20T13:14:59.514930Z"
    }
   },
   "outputs": [],
   "source": [
    "data = data.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T13:14:59.683838Z",
     "start_time": "2020-05-20T13:14:59.564307Z"
    }
   },
   "outputs": [],
   "source": [
    "data.createTempView('df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T13:15:14.708726Z",
     "start_time": "2020-05-20T13:15:14.669341Z"
    }
   },
   "outputs": [],
   "source": [
    "data = truncate_term(data, 'term')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
